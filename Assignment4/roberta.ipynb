{"cells":[{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:24.964213Z","iopub.status.busy":"2024-04-12T18:07:24.963865Z","iopub.status.idle":"2024-04-12T18:07:43.973933Z","shell.execute_reply":"2024-04-12T18:07:43.973110Z","shell.execute_reply.started":"2024-04-12T18:07:24.964177Z"},"trusted":true},"outputs":[],"source":["import torch\n","from datasets import load_dataset\n","from transformers import (\n","    RobertaTokenizerFast,\n","    RobertaForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    AutoConfig,\n",")\n","# from sklearnex import patch_sklearn\n","# patch_sklearn()\n","\n","import gc\n","import json\n","import pickle\n","import numpy as np    \n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:43.976062Z","iopub.status.busy":"2024-04-12T18:07:43.975491Z","iopub.status.idle":"2024-04-12T18:07:44.009321Z","shell.execute_reply":"2024-04-12T18:07:44.008253Z","shell.execute_reply.started":"2024-04-12T18:07:43.976034Z"},"trusted":true},"outputs":[],"source":["PATH='D:\\\\ghd\\\\NLP-Assignments\\\\Assignment4\\\\data\\\\'\n","OUTPATH='D:\\\\ghd\\\\NLP-Assignments\\\\Assignment4\\\\output\\\\'\n","BATCH_SIZE=4\n","MAX_LENGTH=128\n","MAX_UTTERANCES=25\n","EPOCHS=1\n","EOS='</s>'\n","SEP='[SEP]'\n","SOS='<s>'\n","\n","torch.manual_seed(0)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_id = \"roberta-base\"\n","\n","label_encoding ={\n","    'x':0,\n","    'surprise':1,\n","    'sadness':2,\n","    'anger':3,\n","    'fear':4,\n","    'disgust':5,\n","    'joy':6,\n","    'neutral':7\n","}"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:44.010637Z","iopub.status.busy":"2024-04-12T18:07:44.010355Z","iopub.status.idle":"2024-04-12T18:07:44.157315Z","shell.execute_reply":"2024-04-12T18:07:44.156530Z","shell.execute_reply.started":"2024-04-12T18:07:44.010611Z"},"trusted":true},"outputs":[],"source":["# read json\n","def get_data(file_path):\n","    global label_encoding\n","    with open(file_path) as f:\n","        data = json.load(f)\n","        # to pandas\n","        df = pd.DataFrame(data)\n","\n","    sentence_len = []\n","    text=[]\n","    text_len=[]\n","\n","    for i in range(len(df)):\n","        uterances=df.iloc[i]['utterances']\n","        speaker=df.iloc[i]['speakers']\n","        length=0\n","        sentences=[]\n","        for (speaker,uterance) in zip(speaker,uterances):\n","            sentence=speaker+': '+uterance\n","            sentences.append(sentence)\n","            length+=len(uterance.split())\n","        if(length==2):\n","            print(sentence)\n","            print(df.iloc[i])\n","        sentence_len.append(length)\n","        text_len.append(len(sentences))\n","        text.append(sentences)\n","        # print(length)\n","        # break\n","    y=[]\n","    for i in df[\"emotions\"]:\n","        lst=[]\n","        for j in i:\n","            lst.append(label_encoding[j])\n","        if(len(lst)>MAX_UTTERANCES):\n","            lst=lst[:MAX_UTTERANCES]\n","        else:\n","            lst.extend([0]*(MAX_UTTERANCES-len(lst)))\n","        y.append(lst)\n","    x=[]\n","    for i in text:\n","        lst=[]\n","        for j in i:\n","            lst.append(j)\n","        if(len(lst)>MAX_UTTERANCES):\n","            lst=lst[:MAX_UTTERANCES]\n","        else:\n","            lst.extend([EOS]*(MAX_UTTERANCES-len(lst)))\n","        # print(lst)\n","        # print(len(lst))\n","        x.append(lst)\n","\n","    return x[:100],y[:100]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:44.185301Z","iopub.status.busy":"2024-04-12T18:07:44.185076Z","iopub.status.idle":"2024-04-12T18:07:44.757644Z","shell.execute_reply":"2024-04-12T18:07:44.756820Z","shell.execute_reply.started":"2024-04-12T18:07:44.185280Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Phoebe: No!\n","episode          utterance_915\n","speakers      [Phoebe, Phoebe]\n","emotions      [neutral, anger]\n","utterances          [No., No!]\n","triggers            [0.0, 0.0]\n","Name: 968, dtype: object\n","Phoebe: No!\n","episode          utterance_915\n","speakers      [Phoebe, Phoebe]\n","emotions      [neutral, anger]\n","utterances          [No., No!]\n","triggers            [0.0, 0.0]\n","Name: 3984, dtype: object\n"]}],"source":["x_train, y_train = get_data(PATH+'train_file.json')\n","x_val, y_val = get_data(PATH+'val_file.json')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["(['Rachel: They made you head of the department!',\n","  \"Ross: No, I get to teach one of his advanced classes!  Why didn't I get head of the department?\",\n","  'Joey: Oh! Hey Rach, listen umm',\n","  'Rachel: Yeah.',\n","  'Joey: I got a big date coming up, do you know a good restaurant?',\n","  \"Rachel: Uh, Paul's Caf√©. They got great food and it's really romantic.\",\n","  'Joey: Ooh, great! Thanks!',\n","  'Rachel: Yeah! Oh, and then afterwards you can take her to the',\n","  \"Joey: You sure are naming a lot of ways to postpone sex, I'll tell ya\",\n","  'Rachel: Ooh, I miss dating.',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>',\n","  '</s>'],\n"," [6, 6, 7, 7, 7, 7, 6, 7, 7, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["ii=17\n","x_train[ii],y_train[ii]"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:19:54.884957Z","iopub.status.busy":"2024-04-12T18:19:54.884085Z","iopub.status.idle":"2024-04-12T18:19:56.022999Z","shell.execute_reply":"2024-04-12T18:19:56.022221Z","shell.execute_reply.started":"2024-04-12T18:19:54.884924Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:54.530403Z","iopub.status.busy":"2024-04-12T18:07:54.530161Z","iopub.status.idle":"2024-04-12T18:07:54.540309Z","shell.execute_reply":"2024-04-12T18:07:54.539573Z","shell.execute_reply.started":"2024-04-12T18:07:54.530381Z"},"trusted":true},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, data, tokenizer,labels):\n","        self.data=data\n","        self.tokenizer = tokenizer\n","        self.labels = labels\n","    \n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):   \n","        encoded_pair = self.tokenizer(self.data[idx],max_length=MAX_LENGTH,truncation=True,return_tensors=\"pt\",padding=\"max_length\")\n","        # print(encoded_pair)\n","        input_ids = encoded_pair['input_ids'].squeeze(0)\n","        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n","        return input_ids,attention_mask,torch.tensor(self.labels[idx])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:54.541806Z","iopub.status.busy":"2024-04-12T18:07:54.541490Z","iopub.status.idle":"2024-04-12T18:07:54.554900Z","shell.execute_reply":"2024-04-12T18:07:54.553954Z","shell.execute_reply.started":"2024-04-12T18:07:54.541776Z"},"trusted":true},"outputs":[],"source":["train_dataset = Dataset(x_train,tokenizer,y_train)\n","val_dataset = Dataset(x_val,tokenizer,y_val)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:54.556440Z","iopub.status.busy":"2024-04-12T18:07:54.556088Z","iopub.status.idle":"2024-04-12T18:07:54.572774Z","shell.execute_reply":"2024-04-12T18:07:54.571937Z","shell.execute_reply.started":"2024-04-12T18:07:54.556416Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["25\n","25\n","25\n"]}],"source":["for i in train_dataset:\n","    for j in i:\n","        print(len(j))\n","    break"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:25:26.937861Z","iopub.status.busy":"2024-04-12T18:25:26.937491Z","iopub.status.idle":"2024-04-12T18:25:26.942922Z","shell.execute_reply":"2024-04-12T18:25:26.941927Z","shell.execute_reply.started":"2024-04-12T18:25:26.937831Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","train_dataloader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=False)\n","val_dataloader=DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["Roberta output should be of size BATCH_SIZExlabels, we need linear layer to output BATCH_SIZExlabels as emotions in one way or another after we will do softmax. Now manage this. "]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:32:52.299253Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["\n","class EmotionClassifier(nn.Module):\n","    def __init__(self, roberta_model, roberta_labels,num_labels):\n","        super(EmotionClassifier, self).__init__()\n","        self.roberta_labels = roberta_labels\n","        self.num_labels = num_labels\n","        self.roberta = roberta_model\n","        self.roberta.requires_grad_(False)  \n","        for param in self.roberta.roberta.encoder.layer[-3:].parameters():\n","            param.requires_grad = True\n","\n","        self.linear = nn.Linear(MAX_UTTERANCES*roberta_labels, MAX_UTTERANCES*num_labels) \n","        \n","    def forward(self, input_ids, attention_mask, token_type_ids=None):\n","        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids)\n","        output_logits=outputs.logits # output: (50,10)\n","        output_logits=output_logits.view(BATCH_SIZE,MAX_UTTERANCES*self.roberta_labels) # (2,25*10)\n","        # print(\"OUTPUT: \",output_logits.shape) # now working correctly\n","        logits = self.linear(output_logits)\n","        logits=logits.view(BATCH_SIZE,MAX_UTTERANCES,self.num_labels)\n","        # print(\"logits: \",logits.shape)\n","        softmax_output = nn.functional.softmax(logits, dim=-1)\n","        # print(\"softmax: \",softmax_output.shape)\n","        return softmax_output\n","    \n","num_labels = 8\n","roberta_labels = 10\n","config = AutoConfig.from_pretrained(model_id)\n","config.num_labels = roberta_labels\n","roberta_model = RobertaForSequenceClassification.from_pretrained(model_id,config=config)\n","model = EmotionClassifier(roberta_model,roberta_labels, num_labels)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(),lr=0.005)\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","train_losses=[]\n","val_losses=[]\n"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":true},"outputs":[],"source":["\n","\n","def train_epoch(model, optimizer,epoch):\n","    model.train()\n","    losses = 0\n","    for batch in tqdm(train_dataloader, desc=f\"Epoch:{epoch}\",total=len(train_dataloader), leave=False):\n","        input_ids = batch[0].to(device)\n","        attention_mask = batch[1].to(device)\n","        # token_type_ids = batch[2].to(device)\n","        labels = batch[2].to(device)\n","        # print(\"train: \",labels.shape,input_ids.shape,attention_mask.shape)\n","        utterance_wise_batch={}\n","        utterance_input_ids=input_ids.view(MAX_UTTERANCES*BATCH_SIZE,-1)\n","        utterance_attention_mask=attention_mask.view(MAX_UTTERANCES*BATCH_SIZE,-1)\n","        # for i in range(BATCH_SIZE):\n","        #     for j in range(MAX_UTTERANCES):\n","        #         utterance_input_ids.append(input_ids[i][j])\n","        #         utterance_attention_mask.append(attention_mask[i][j])\n","        # print(len(utterance_wise_batch),utterance_wise_batch[0])\n","        # utterance_input_ids=torch.stack(utterance_input_ids)\n","        # utterance_attention_mask=torch.stack(utterance_attention_mask)\n","        utterance_wise_batch['input_ids']=utterance_input_ids\n","        utterance_wise_batch['attention_mask']=utterance_attention_mask\n","        # print(utterance_input_ids.shape,utterance_attention_mask.shape,len(utterance_wise_batch))\n","        optimizer.zero_grad()\n","        predicted = model(utterance_input_ids, utterance_attention_mask)\n","        # print(\"lables: \",labels.shape)\n","        predicted=predicted.view(-1,num_labels)\n","        labels=labels.view(-1)\n","        loss = criterion(predicted, labels)\n","        losses += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        del input_ids\n","        del attention_mask\n","        del labels\n","        del predicted\n","        del utterance_input_ids\n","        del utterance_attention_mask\n","        del utterance_wise_batch\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # break\n","    x = losses / len(list(train_dataloader))\n","    train_losses.append(x)\n","#     wandb.log({'epoch':epoch,'train_loss':x})\n","    tqdm.write(f\"Epoch:{epoch}, Avg Train Loss: {x}\")\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    return x\n","\n","\n","\n","def evaluate(model,val_dataloader,name):\n","    with torch.no_grad():\n","        model.eval()\n","        losses = 0\n","\n","        all_labels = []\n","        for batch in tqdm(val_dataloader, desc=name,total=len(val_dataloader), leave=False):\n","            input_ids = batch[0].to(device)\n","            attention_mask = batch[1].to(device)\n","            labels = batch[2].to(device)\n","            utterance_wise_batch={}\n","            utterance_input_ids=input_ids.view(MAX_UTTERANCES*BATCH_SIZE,-1)\n","            utterance_attention_mask=attention_mask.view(MAX_UTTERANCES*BATCH_SIZE,-1)\n","            utterance_wise_batch['input_ids']=utterance_input_ids\n","            utterance_wise_batch['attention_mask']=utterance_attention_mask\n","            predicted = model(utterance_input_ids, utterance_attention_mask)\n","            predicted=predicted.view(-1,num_labels)\n","            labels=labels.view(-1)\n","            loss = criterion(predicted, labels)\n","            losses += loss.item()\n","            predicted_labels = predicted.argmax(dim=-1)\n","            all_labels.extend(predicted_labels.cpu().numpy())\n","            del input_ids\n","            del attention_mask\n","            del labels\n","            del predicted\n","            del utterance_input_ids\n","            del utterance_attention_mask\n","            del utterance_wise_batch\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            # break\n","        x = losses / len(list(val_dataloader))\n","        val_losses.append(x)\n","        tqdm.write(f\"Avg {name} Loss: {x}\")\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        return x, all_labels\n","    \n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                        \r"]},{"name":"stdout","output_type":"stream","text":["Epoch:1, Avg Train Loss: 1.529209361076355\n"]},{"name":"stderr","output_type":"stream","text":["                                                    \r"]},{"name":"stdout","output_type":"stream","text":["Avg Val Loss: 1.5720089387893676\n","Epoch: 1, Train loss: 1.529, Val loss: 1.572\n"]}],"source":["gc.collect()\n","torch.cuda.empty_cache()\n","for epoch in range(1, EPOCHS+1):\n","    train_loss = train_epoch(model, optimizer,epoch)\n","    val_loss,all_labels = evaluate(model,val_dataloader=val_dataloader,name='Val')\n","    torch.save(model, f\"{OUTPATH}modelM1_epoch{epoch}.pth\")\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\"))"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["torch.save(model, f\"{OUTPATH}modelM1.pth\")\n","torch.save(tokenizer, f\"{OUTPATH}tokenizerM1.pth\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                     \r"]},{"name":"stdout","output_type":"stream","text":["Avg Test Loss: 1.5720089387893676\n"]}],"source":["# load model\n","loaded_model = torch.load(f\"{OUTPATH}modelM1.pth\")\n","loaded_tokenizer = torch.load(f\"{OUTPATH}tokenizerM1.pth\")\n","\n","# test\n","x_test, y_test = get_data(PATH+'val_file.json')\n","test_dataset = Dataset(x_test,loaded_tokenizer,y_test)\n","test_dataloader=DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False)\n","test_loss,all_labels = evaluate(loaded_model,test_dataloader,name='Test')\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test loss: 1.572\n"]},{"ename":"ValueError","evalue":"Found input variables with inconsistent numbers of samples: [100, 2500]","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m f1_sccore \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweighted\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m f1_macro \u001b[38;5;241m=\u001b[39m f1_score(y_test, all_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(f1_macro,f1_sccore)\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1238\u001b[0m, in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1071\u001b[0m     {\n\u001b[0;32m   1072\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1096\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1097\u001b[0m ):\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \n\u001b[0;32m   1100\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfbeta_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1411\u001b[0m, in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m   1251\u001b[0m     {\n\u001b[0;32m   1252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1279\u001b[0m ):\n\u001b[0;32m   1280\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[0;32m   1281\u001b[0m \n\u001b[0;32m   1282\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;124;03m    0.38...\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1411\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1413\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf-score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:184\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    182\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1721\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[0;32m   1564\u001b[0m \n\u001b[0;32m   1565\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1718\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m zero_division_value \u001b[38;5;241m=\u001b[39m _check_zero_division(zero_division)\n\u001b[1;32m-> 1721\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1723\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[0;32m   1724\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1499\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m average \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m average_options \u001b[38;5;129;01mand\u001b[39;00m average \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1497\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage has to be one of \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(average_options))\n\u001b[1;32m-> 1499\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;66;03m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m present_labels \u001b[38;5;241m=\u001b[39m unique_labels(y_true, y_pred)\u001b[38;5;241m.\u001b[39mtolist()\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n","\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [100, 2500]"]}],"source":["\n","print(f\"Test loss: {test_loss:.3f}\")\n","f1_sccore = f1_score(y_test, all_labels, average='weighted')\n","f1_macro = f1_score(y_test, all_labels, average='macro')\n","print(f1_macro,f1_sccore)\n","print(classification_report(y_test, all_labels))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4784361,"sourceId":8101413,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
