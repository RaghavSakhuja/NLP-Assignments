{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:24.964213Z","iopub.status.busy":"2024-04-12T18:07:24.963865Z","iopub.status.idle":"2024-04-12T18:07:43.973933Z","shell.execute_reply":"2024-04-12T18:07:43.973110Z","shell.execute_reply.started":"2024-04-12T18:07:24.964177Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["import torch\n","from datasets import load_dataset\n","from transformers import (\n","    RobertaTokenizerFast,\n","    RobertaForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    AutoConfig,\n",")\n","# from sklearnex import patch_sklearn\n","# patch_sklearn()\n","\n","import json\n","import pandas as pd\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","import pickle\n","import numpy as np\n","from sklearn.metrics import balanced_accuracy_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import classification_report\n","\n","\n","max_length=128"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:43.976062Z","iopub.status.busy":"2024-04-12T18:07:43.975491Z","iopub.status.idle":"2024-04-12T18:07:44.009321Z","shell.execute_reply":"2024-04-12T18:07:44.008253Z","shell.execute_reply.started":"2024-04-12T18:07:43.976034Z"},"trusted":true},"outputs":[],"source":["PATH='train_file.json'\n","BATCH_SIZE=16\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_id = \"roberta-base\"\n","label_encoder = LabelEncoder()"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:44.010637Z","iopub.status.busy":"2024-04-12T18:07:44.010355Z","iopub.status.idle":"2024-04-12T18:07:44.157315Z","shell.execute_reply":"2024-04-12T18:07:44.156530Z","shell.execute_reply.started":"2024-04-12T18:07:44.010611Z"},"trusted":true},"outputs":[],"source":["# read json\n","with open(PATH) as f:\n","    data = json.load(f)\n","    # to pandas\n","    df = pd.DataFrame(data)\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:44.159617Z","iopub.status.busy":"2024-04-12T18:07:44.159326Z","iopub.status.idle":"2024-04-12T18:07:44.184210Z","shell.execute_reply":"2024-04-12T18:07:44.183365Z","shell.execute_reply.started":"2024-04-12T18:07:44.159592Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>episode</th>\n","      <th>speakers</th>\n","      <th>emotions</th>\n","      <th>utterances</th>\n","      <th>triggers</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>utterance_3492</td>\n","      <td>[Phoebe, Eric, Phoebe, Eric, Phoebe]</td>\n","      <td>[surprise, fear, surprise, sadness, disgust]</td>\n","      <td>[You-youyou had sex with Ursula?!, Uh, a litt...</td>\n","      <td>[1.0, 1.0, 0.0, 0.0, 0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>utterance_3952</td>\n","      <td>[Monica, Monica, Phoebe, Joey, Joey, Joey, Rac...</td>\n","      <td>[disgust, disgust, anger, sadness, surprise, a...</td>\n","      <td>[Dad, please don't pick your teeth out here!, ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>utterance_3198</td>\n","      <td>[Older Scientist, Ross, Ross, Joey, Ross, Ross...</td>\n","      <td>[neutral, neutral, neutral, neutral, neutral, ...</td>\n","      <td>[Dr. Geller, there's a seat over here., Thank ...</td>\n","      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>utterance_2834</td>\n","      <td>[Monica, Monica, Monica]</td>\n","      <td>[neutral, surprise, neutral]</td>\n","      <td>[So, how'd the lasagne go over?, Really?!, Good.]</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>utterance_453</td>\n","      <td>[Kate, The Director, Kate]</td>\n","      <td>[joy, sadness, sadness]</td>\n","      <td>[Become a drama critic!, I am hurt!  A plague ...</td>\n","      <td>[0.0, 0.0, 1.0]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          episode                                           speakers  \\\n","0  utterance_3492               [Phoebe, Eric, Phoebe, Eric, Phoebe]   \n","1  utterance_3952  [Monica, Monica, Phoebe, Joey, Joey, Joey, Rac...   \n","2  utterance_3198  [Older Scientist, Ross, Ross, Joey, Ross, Ross...   \n","3  utterance_2834                           [Monica, Monica, Monica]   \n","4   utterance_453                         [Kate, The Director, Kate]   \n","\n","                                            emotions  \\\n","0       [surprise, fear, surprise, sadness, disgust]   \n","1  [disgust, disgust, anger, sadness, surprise, a...   \n","2  [neutral, neutral, neutral, neutral, neutral, ...   \n","3                       [neutral, surprise, neutral]   \n","4                            [joy, sadness, sadness]   \n","\n","                                          utterances  \\\n","0  [You-you\n","you had sex with Ursula?!, Uh, a litt...   \n","1  [Dad, please don't pick your teeth out here!, ...   \n","2  [Dr. Geller, there's a seat over here., Thank ...   \n","3  [So, how'd the lasagne go over?, Really?!, Good.]   \n","4  [Become a drama critic!, I am hurt!  A plague ...   \n","\n","                                            triggers  \n","0                          [1.0, 1.0, 0.0, 0.0, 0.0]  \n","1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n","2                [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0]  \n","3                                    [0.0, 0.0, 1.0]  \n","4                                    [0.0, 0.0, 1.0]  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:44.185301Z","iopub.status.busy":"2024-04-12T18:07:44.185076Z","iopub.status.idle":"2024-04-12T18:07:44.757644Z","shell.execute_reply":"2024-04-12T18:07:44.756820Z","shell.execute_reply.started":"2024-04-12T18:07:44.185280Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> Phoebe:No. [SEP] Phoebe:No! [SEP] \n","episode          utterance_915\n","speakers      [Phoebe, Phoebe]\n","emotions      [neutral, anger]\n","utterances          [No., No!]\n","triggers            [0.0, 0.0]\n","Name: 968, dtype: object\n","<s> Phoebe:No. [SEP] Phoebe:No! [SEP] \n","episode          utterance_915\n","speakers      [Phoebe, Phoebe]\n","emotions      [neutral, anger]\n","utterances          [No., No!]\n","triggers            [0.0, 0.0]\n","Name: 3984, dtype: object\n"]}],"source":["sentence_len = []\n","text=[]\n","EOS='</s>'\n","SEP='[SEP]'\n","SOS='<s>'\n","\n","for i in range(len(df)):\n","    uterances=df.iloc[i]['utterances']\n","    speaker=df.iloc[i]['speakers']\n","    length=0\n","    sentence=SOS+' '\n","    for (speaker,uterance) in zip(speaker,uterances):\n","        sentence+=speaker+':'+uterance+' '+SEP+' '\n","\n","        length+=len(uterance.split())\n","    if(length==2):\n","        print(sentence)\n","        print(df.iloc[i])\n","    sentence_len.append(length)\n","    text.append(sentence+EOS)\n","    # print(length)\n","    # break"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:44.758841Z","iopub.status.busy":"2024-04-12T18:07:44.758603Z","iopub.status.idle":"2024-04-12T18:07:44.762827Z","shell.execute_reply":"2024-04-12T18:07:44.761845Z","shell.execute_reply.started":"2024-04-12T18:07:44.758819Z"},"trusted":true},"outputs":[],"source":["# config =AutoConfig.from_pretrained(model_id)\n","# config.update({\"id2label\": label_to_id})\n","# tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n","# model = RobertaForSequenceClassification.from_pretrained(model_id,config=config)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:44.764431Z","iopub.status.busy":"2024-04-12T18:07:44.763888Z","iopub.status.idle":"2024-04-12T18:07:44.784495Z","shell.execute_reply":"2024-04-12T18:07:44.783525Z","shell.execute_reply.started":"2024-04-12T18:07:44.764406Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["6740 4179\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;s&gt; Phoebe:You-youyou had sex with Ursula?! [...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;s&gt; Monica:Dad, please don't pick your teeth o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>&lt;s&gt; Older Scientist:Dr. Geller, there's a seat...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>&lt;s&gt; Monica:So, how'd the lasagne go over? [SEP...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>&lt;s&gt; Kate:Become a drama critic! [SEP] The Dire...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text\n","0  <s> Phoebe:You-you\n","you had sex with Ursula?! [...\n","1  <s> Monica:Dad, please don't pick your teeth o...\n","2  <s> Older Scientist:Dr. Geller, there's a seat...\n","3  <s> Monica:So, how'd the lasagne go over? [SEP...\n","4  <s> Kate:Become a drama critic! [SEP] The Dire..."]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["text_df=pd.DataFrame(text,columns=['text'])\n","print(len(text),len(set(text)))\n","text_df.head()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:44.785819Z","iopub.status.busy":"2024-04-12T18:07:44.785566Z","iopub.status.idle":"2024-04-12T18:07:44.793205Z","shell.execute_reply":"2024-04-12T18:07:44.792263Z","shell.execute_reply.started":"2024-04-12T18:07:44.785798Z"},"trusted":true},"outputs":[],"source":["label_encoding ={\n","    'x':0,\n","    'surprise':1,\n","    'sadness':2,\n","    'anger':3,\n","    'fear':4,\n","    'disgust':5,\n","    'joy':6,\n","    'neutral':7\n","}\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:44.794482Z","iopub.status.busy":"2024-04-12T18:07:44.794234Z","iopub.status.idle":"2024-04-12T18:07:44.842772Z","shell.execute_reply":"2024-04-12T18:07:44.842106Z","shell.execute_reply.started":"2024-04-12T18:07:44.794460Z"},"trusted":true},"outputs":[],"source":["y_train=[]\n","for i in df[\"emotions\"]:\n","    lst=[]\n","    for j in i:\n","        lst.append(label_encoding[j])\n","\n","    lst.extend([0]*(max_length-len(lst)))\n","    y_train.append(lst)\n","x_train=text_df['text']"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:44.845742Z","iopub.status.busy":"2024-04-12T18:07:44.845475Z","iopub.status.idle":"2024-04-12T18:07:44.855951Z","shell.execute_reply":"2024-04-12T18:07:44.855104Z","shell.execute_reply.started":"2024-04-12T18:07:44.845719Z"},"trusted":true},"outputs":[{"data":{"text/plain":["([1,\n","  4,\n","  1,\n","  2,\n","  5,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0],\n"," [5,\n","  5,\n","  3,\n","  2,\n","  1,\n","  3,\n","  7,\n","  3,\n","  3,\n","  3,\n","  3,\n","  4,\n","  7,\n","  6,\n","  3,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0])"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["y_train[0],y_train[1]"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:19:54.884957Z","iopub.status.busy":"2024-04-12T18:19:54.884085Z","iopub.status.idle":"2024-04-12T18:19:56.022999Z","shell.execute_reply":"2024-04-12T18:19:56.022221Z","shell.execute_reply.started":"2024-04-12T18:19:54.884924Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from torch import nn\n","config = AutoConfig.from_pretrained(model_id)\n","tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n","config.update({\"id2label\": label_encoding})\n","roberta_model = RobertaForSequenceClassification.from_pretrained(model_id,config=config)\n"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:53.778664Z","iopub.status.busy":"2024-04-12T18:07:53.778373Z","iopub.status.idle":"2024-04-12T18:07:53.782683Z","shell.execute_reply":"2024-04-12T18:07:53.781662Z","shell.execute_reply.started":"2024-04-12T18:07:53.778618Z"},"trusted":true},"outputs":[],"source":["# x_train_tokenized = tokenizer(text_df[\"text\"].tolist(),padding=True,truncation=True,max_length=128)\n","# x_train_tokenized[0]"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:53.784830Z","iopub.status.busy":"2024-04-12T18:07:53.783821Z","iopub.status.idle":"2024-04-12T18:07:54.523324Z","shell.execute_reply":"2024-04-12T18:07:54.522312Z","shell.execute_reply.started":"2024-04-12T18:07:53.784792Z"},"trusted":true},"outputs":[],"source":["# model = model.to(device)\n","# model"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:54.524798Z","iopub.status.busy":"2024-04-12T18:07:54.524443Z","iopub.status.idle":"2024-04-12T18:07:54.529216Z","shell.execute_reply":"2024-04-12T18:07:54.528377Z","shell.execute_reply.started":"2024-04-12T18:07:54.524765Z"},"trusted":true},"outputs":[],"source":["# class Dataset(torch.utils.data.Dataset):\n","#     def __init__(self,encodings,labels=None):\n","#         self.labels = labels\n","#         self.encodings = encodings\n","    \n","#     def __getitem__(self,idx):\n","#         item = {key:torch.tensor(val[idx]) for key,val in self.encodings.items()}\n","#         if self.labels is not None:\n","#             item[\"labels\"]  = torch.tensor(self.labels[idx])\n","#         return item\n","# #             return input_data[\"input_ids\"],input_data[\"attention_mask\"],self.labels[idx]\n","\n","\n","#     def __len__(self):\n","#         return len(self.encodings[\"input_ids\"])"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:54.530403Z","iopub.status.busy":"2024-04-12T18:07:54.530161Z","iopub.status.idle":"2024-04-12T18:07:54.540309Z","shell.execute_reply":"2024-04-12T18:07:54.539573Z","shell.execute_reply.started":"2024-04-12T18:07:54.530381Z"},"trusted":true},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, data, tokenizer,labels):\n","        self.data=data\n","        self.tokenizer = tokenizer\n","        self.labels = labels\n","    \n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):   \n","        encoded_pair = self.tokenizer(self.data[idx],max_length=128,truncation=True,return_tensors=\"pt\",padding=\"max_length\")\n","        # print(encoded_pair)\n","        input_ids = encoded_pair['input_ids'].squeeze(0)\n","        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n","        # token_type_ids = encoded_pair['token_type_ids'].squeeze(0)\n","        return input_ids,attention_mask,torch.tensor(self.labels[idx])"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:54.541806Z","iopub.status.busy":"2024-04-12T18:07:54.541490Z","iopub.status.idle":"2024-04-12T18:07:54.554900Z","shell.execute_reply":"2024-04-12T18:07:54.553954Z","shell.execute_reply.started":"2024-04-12T18:07:54.541776Z"},"trusted":true},"outputs":[],"source":["train_dataset = Dataset(x_train,tokenizer,y_train)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:07:54.556440Z","iopub.status.busy":"2024-04-12T18:07:54.556088Z","iopub.status.idle":"2024-04-12T18:07:54.572774Z","shell.execute_reply":"2024-04-12T18:07:54.571937Z","shell.execute_reply.started":"2024-04-12T18:07:54.556416Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["128\n","128\n","128\n"]}],"source":["for i in train_dataset:\n","    for j in i:\n","        print(len(j))\n","    break"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-04-12T18:25:26.937861Z","iopub.status.busy":"2024-04-12T18:25:26.937491Z","iopub.status.idle":"2024-04-12T18:25:26.942922Z","shell.execute_reply":"2024-04-12T18:25:26.941927Z","shell.execute_reply.started":"2024-04-12T18:25:26.937831Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","train_dataloader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["Roberta output should be of size BATCH_SIZExlabels, we need linear layer to output BATCH_SIZExlabels as emotions in one way or another after we will do softmax. Now manage this. "]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.status.busy":"2024-04-12T18:32:52.299253Z"},"trusted":true},"outputs":[],"source":["\n","class EmotionClassifier(nn.Module):\n","    def __init__(self, roberta_model, num_labels):\n","        super(EmotionClassifier, self).__init__()\n","        self.roberta = roberta_model\n","        self.linear = nn.Linear(BATCH_SIZE, num_labels) # edit this to match out desirec size\n","        self.Softmax = nn.Softmax() \n","\n","    def forward(self, input_ids, attention_mask, token_type_ids=None):\n","        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids)\n","        print(outputs.logits.shape) # now working correctly\n","        pooled_output = outputs['logits']  \n","        logits = self.linear(pooled_output)\n","        probabilities = self.Softmax(logits)\n","        return probabilities"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":50,"metadata":{"trusted":true},"outputs":[],"source":["num_labels = 8\n","model = EmotionClassifier(roberta_model, num_labels)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(),lr=0.005)\n","criterion = nn.CrossEntropyLoss()\n","\n","from tqdm import tqdm\n","import gc\n","train_losses=[]\n","val_losses=[]\n","def train_epoch(model, optimizer,epoch):\n","    model.train()\n","    losses = 0\n","    progress = tqdm(train_dataloader, desc=f\"Epoch:{epoch}\",total=len(train_dataloader), leave=False)\n","    i=0\n","    for batch in progress:\n","        input_ids = batch[0].to(device)\n","        attention_mask = batch[1].to(device)\n","        # token_type_ids = batch[2].to(device)\n","        labels = batch[2].to(device)\n","        optimizer.zero_grad()\n","        probabilities = model(input_ids, attention_mask=attention_mask)\n","        loss = criterion(probabilities, labels.float())\n","        losses += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        del input_ids\n","        del attention_mask\n","        del labels\n","        del logits\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        progress.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n","        break\n","    x = losses / len(list(train_dataloader))\n","    train_losses.append(x)\n","#     wandb.log({'epoch':epoch,'train_loss':x})\n","    tqdm.write(f\"Epoch:{epoch}, Avg Train Loss: {x}\")\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    return x\n","\n","\n","\n","def evaluate(model,val_dataloader):\n","    with torch.no_grad():\n","        model.eval()\n","        losses = 0\n","        \n","        total_logits=[]\n","        for batch in val_dataloader:\n","            input_ids = batch[0].to(device)\n","            attention_mask = batch[1].to(device)\n","            logits=model(input_ids,attention_mask=attention_mask).logits.to(torch.float64).view(-1)*5\n","            total_logits.extend(list(logits.cpu().detach().numpy()))\n","            \n","        return total_logits"]},{"cell_type":"code","execution_count":51,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                \r"]},{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (16x8 and 16x8)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[51], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate(model)\n\u001b[0;32m      6\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel1A_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[1;32mIn[50], line 22\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, epoch)\u001b[0m\n\u001b[0;32m     20\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(probabilities, labels\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     24\u001b[0m losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[49], line 12\u001b[0m, in \u001b[0;36mEmotionClassifier.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# print(outputs.logits.shape,outputs)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Extract [CLS] token representation\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSoftmax(logits)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probabilities\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x8 and 16x8)"]}],"source":["gc.collect()\n","torch.cuda.empty_cache()\n","for epoch in range(1, 2):\n","    train_loss = train_epoch(model, optimizer,epoch)\n","    val_loss = evaluate(model)\n","    torch.save(model.state_dict(), f\"model1A_epoch{epoch}.pth\")\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\"))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4784361,"sourceId":8101413,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
