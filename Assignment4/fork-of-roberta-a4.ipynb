{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:09.358190Z","iopub.status.busy":"2024-04-13T11:55:09.357410Z","iopub.status.idle":"2024-04-13T11:55:09.370293Z","shell.execute_reply":"2024-04-13T11:55:09.369101Z","shell.execute_reply.started":"2024-04-13T11:55:09.358161Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["import torch\n","from torch import nn\n","from datasets import load_dataset\n","from transformers import (\n","    RobertaTokenizerFast,\n","    RobertaForSequenceClassification,\n","    TrainingArguments,\n","    Trainer,\n","    AutoConfig,\n",")\n","# from sklearnex import patch_sklearn\n","# patch_sklearn()\n","\n","import gc\n","import json\n","import pickle\n","import numpy as np    \n","import pandas as pd\n","from tqdm.notebook import tqdm\n","from sklearn.metrics import accuracy_score, f1_score, classification_report\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:09.372689Z","iopub.status.busy":"2024-04-13T11:55:09.371981Z","iopub.status.idle":"2024-04-13T11:55:09.381878Z","shell.execute_reply":"2024-04-13T11:55:09.381005Z","shell.execute_reply.started":"2024-04-13T11:55:09.372614Z"},"trusted":true},"outputs":[],"source":["PATH='data/'\n","OUTPATH='output/'\n","BATCH_SIZE=10\n","MAX_LENGTH=128\n","MAX_UTTERANCES=25\n","ROBERTA_LABELS=8\n","EPOCHS=2\n","EOS='</s>'\n","SEP='[SEP]'\n","SOS='<s>'\n","\n","torch.manual_seed(0)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_id = \"roberta-base\"\n","\n","label_encoding ={\n","    'x':0,\n","    'surprise':1,\n","    'sadness':2,\n","    'anger':3,\n","    'fear':4,\n","    'disgust':5,\n","    'joy':6,\n","    'neutral':7\n","}"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"unhashable type: 'list'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[36], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(lst[i][j])\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m     12\u001b[0m             lst[i][j]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(lst[i][j])\n\u001b[1;32m---> 13\u001b[0m s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(s))\n","\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"]}],"source":["# {\"episode\":\"utterance_3492\",\"speakers\":[\"Phoebe\",\"Eric\",\"Phoebe\",\"Eric\",\"Phoebe\"],\"emotions\":[\"surprise\",\"fear\",\"surprise\",\"sadness\",\"disgust\"],\"utterances\":[\"You-you\\u0085you had sex with Ursula?!\",\"Uh, a little bit. She-she-she walked in and I thought she was you and I kissed her and\",\"You didn't notice she was wearing different clothes?!\",\"Well I was just so excited to see you.\",\"Oh. Ew! Ew! Ew! Ugh! Y'know what? This is too weird.\"],\"triggers\":[1.0,1.0,0.0,0.0,0.0]}\n","\n","with open(\"data/train_file.json\") as f:\n","    data = json.load(f)\n","    # to pandas\n","    df = pd.DataFrame(data)\n","#  df to list \n","lst=df.values.tolist()\n","for i in range(len(lst)):\n","    for j in range(len(lst[i])):\n","        if type(lst[i][j])==list:\n","            lst[i][j]=type(lst[i][j])\n","s=set(lst)\n","print(len(s))"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:09.383780Z","iopub.status.busy":"2024-04-13T11:55:09.383113Z","iopub.status.idle":"2024-04-13T11:55:09.395933Z","shell.execute_reply":"2024-04-13T11:55:09.394941Z","shell.execute_reply.started":"2024-04-13T11:55:09.383756Z"},"trusted":true},"outputs":[],"source":["# read json\n","def get_data(file_path):\n","    global label_encoding\n","    with open(file_path) as f:\n","        data = json.load(f)\n","        # to pandas\n","        df = pd.DataFrame(data)\n","\n","    sentence_len = []\n","    text=[]\n","    text_len=[]\n","\n","    for i in range(len(df)):\n","        uterances=df.iloc[i]['utterances']\n","        speaker=df.iloc[i]['speakers']\n","        length=0\n","        sentences=[]\n","        for (speaker,uterance) in zip(speaker,uterances):\n","            sentence=speaker+': '+uterance\n","            sentences.append(sentence)\n","            length+=len(uterance.split())\n","        if(length==2):\n","            print(sentence)\n","            print(df.iloc[i])\n","        sentence_len.append(length)\n","        text_len.append(len(sentences))\n","        text.append(sentences)\n","        # print(length)\n","        # break\n","    y=[]\n","    for i in df[\"emotions\"]:\n","        lst=[]\n","        for j in i:\n","            lst.append(label_encoding[j])\n","        if(len(lst)>MAX_UTTERANCES):\n","            lst=lst[:MAX_UTTERANCES]\n","        else:\n","            lst.extend([0]*(MAX_UTTERANCES-len(lst)))\n","        y.append(lst)\n","    x=[]\n","    for i in text:\n","        lst=[]\n","        for j in i:\n","            lst.append(j)\n","        if(len(lst)>MAX_UTTERANCES):\n","            lst=lst[:MAX_UTTERANCES]\n","        else:\n","            lst.extend([EOS]*(MAX_UTTERANCES-len(lst)))\n","        # print(lst)\n","        # print(len(lst))\n","        x.append(lst)\n","    \n","    return x[:100],y[:100]\n","    return x,y"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:09.398522Z","iopub.status.busy":"2024-04-13T11:55:09.398184Z","iopub.status.idle":"2024-04-13T11:55:10.172373Z","shell.execute_reply":"2024-04-13T11:55:10.171558Z","shell.execute_reply.started":"2024-04-13T11:55:09.398498Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Phoebe: No!\n","episode          utterance_915\n","speakers      [Phoebe, Phoebe]\n","emotions      [neutral, anger]\n","utterances          [No., No!]\n","triggers            [0.0, 0.0]\n","Name: 968, dtype: object\n","Phoebe: No!\n","episode          utterance_915\n","speakers      [Phoebe, Phoebe]\n","emotions      [neutral, anger]\n","utterances          [No., No!]\n","triggers            [0.0, 0.0]\n","Name: 3984, dtype: object\n"]}],"source":["x_train, y_train = get_data(PATH+\"train_file.json\")\n","x_val, y_val = get_data(PATH+'val_file.json')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:10.173787Z","iopub.status.busy":"2024-04-13T11:55:10.173494Z","iopub.status.idle":"2024-04-13T11:55:10.181164Z","shell.execute_reply":"2024-04-13T11:55:10.180168Z","shell.execute_reply.started":"2024-04-13T11:55:10.173762Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(6740, 6740, 843, 843)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["len(x_train),len(y_train),len(x_val),len(y_val)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:10.182793Z","iopub.status.busy":"2024-04-13T11:55:10.182454Z","iopub.status.idle":"2024-04-13T11:55:10.724525Z","shell.execute_reply":"2024-04-13T11:55:10.723321Z","shell.execute_reply.started":"2024-04-13T11:55:10.182763Z"},"trusted":true},"outputs":[],"source":["tokenizer = RobertaTokenizerFast.from_pretrained(model_id)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:10.726900Z","iopub.status.busy":"2024-04-13T11:55:10.726053Z","iopub.status.idle":"2024-04-13T11:55:10.734327Z","shell.execute_reply":"2024-04-13T11:55:10.733208Z","shell.execute_reply.started":"2024-04-13T11:55:10.726863Z"},"trusted":true},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, data, tokenizer,labels):\n","        self.data=data\n","        self.tokenizer = tokenizer\n","        self.labels = labels\n","    \n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):   \n","        encoded_pair = self.tokenizer(self.data[idx],max_length=MAX_LENGTH,truncation=True,return_tensors=\"pt\",padding=\"max_length\")\n","        # print(encoded_pair)\n","        input_ids = encoded_pair['input_ids'].squeeze(0)\n","        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n","        return input_ids,attention_mask,torch.tensor(self.labels[idx])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:10.735612Z","iopub.status.busy":"2024-04-13T11:55:10.735345Z","iopub.status.idle":"2024-04-13T11:55:10.748612Z","shell.execute_reply":"2024-04-13T11:55:10.747744Z","shell.execute_reply.started":"2024-04-13T11:55:10.735581Z"},"trusted":true},"outputs":[],"source":["train_dataset = Dataset(x_train,tokenizer,y_train)\n","val_dataset = Dataset(x_val,tokenizer,y_val)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:10.749981Z","iopub.status.busy":"2024-04-13T11:55:10.749660Z","iopub.status.idle":"2024-04-13T11:55:10.764601Z","shell.execute_reply":"2024-04-13T11:55:10.763575Z","shell.execute_reply.started":"2024-04-13T11:55:10.749955Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["25\n","25\n","25\n"]}],"source":["for i in train_dataset:\n","    for j in i:\n","        print(len(j))\n","    break"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:10.768035Z","iopub.status.busy":"2024-04-13T11:55:10.767760Z","iopub.status.idle":"2024-04-13T11:55:10.783992Z","shell.execute_reply":"2024-04-13T11:55:10.782963Z","shell.execute_reply.started":"2024-04-13T11:55:10.768012Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","train_dataloader=DataLoader(train_dataset,batch_size=BATCH_SIZE,shuffle=False)\n","val_dataloader=DataLoader(val_dataset,batch_size=BATCH_SIZE,shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["Roberta output should be of size BATCH_SIZExlabels, we need linear layer to output BATCH_SIZExlabels as emotions in one way or another after we will do softmax. Now manage this. "]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:10.786225Z","iopub.status.busy":"2024-04-13T11:55:10.785872Z","iopub.status.idle":"2024-04-13T11:55:11.428182Z","shell.execute_reply":"2024-04-13T11:55:11.427400Z","shell.execute_reply.started":"2024-04-13T11:55:10.786193Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["\n","class EmotionClassifier(nn.Module):\n","    def __init__(self, roberta_model, roberta_labels,num_labels):\n","        super(EmotionClassifier, self).__init__()\n","        self.roberta_labels = roberta_labels\n","        self.num_labels = num_labels\n","        self.roberta = roberta_model\n","        self.roberta.requires_grad_(False)  \n","        for param in self.roberta.roberta.encoder.layer[-2:].parameters():\n","            param.requires_grad = True\n","\n","        # self.linear = nn.Linear(MAX_UTTERANCES*roberta_labels, MAX_UTTERANCES*num_labels)\n","        # lstm\n","        self.lstm = nn.LSTM(input_size=roberta_labels, hidden_size=roberta_labels, num_layers=1, batch_first=True) \n","        \n","    def forward(self, input_ids, attention_mask, token_type_ids=None):\n","        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids)\n","        output_logits=outputs.logits # output: (50,10)\n","        batch_size = output_logits.shape[0]//MAX_UTTERANCES\n","        # print(\"OUTPUT: \",output_logits.shape, batch_size) \n","        output_logits=output_logits.view(batch_size,MAX_UTTERANCES*self.roberta_labels) # (2,25*10)\n","        # print(\"OUTPUT: \",output_logits.shape) \n","        # logits = self.linear(output_logits)\n","        logits=self.lstm(output_logits)\n","        logits=logits.view(batch_size,MAX_UTTERANCES,self.num_labels)\n","        # print(\"logits: \",logits.shape)\n","        softmax_output = nn.functional.softmax(logits, dim=-1)\n","        # print(\"softmax: \",softmax_output.shape)\n","        return softmax_output\n","    \n","num_labels = 8\n","config = AutoConfig.from_pretrained(model_id)\n","config.num_labels = ROBERTA_LABELS\n","roberta_model = RobertaForSequenceClassification.from_pretrained(model_id,config=config)\n","model = EmotionClassifier(roberta_model,ROBERTA_LABELS, num_labels)\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(),lr=0.005)\n","criterion = nn.CrossEntropyLoss()\n","\n","\n","train_losses=[]\n","val_losses=[]\n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:11.430073Z","iopub.status.busy":"2024-04-13T11:55:11.429725Z","iopub.status.idle":"2024-04-13T11:55:11.447466Z","shell.execute_reply":"2024-04-13T11:55:11.446543Z","shell.execute_reply.started":"2024-04-13T11:55:11.430039Z"},"trusted":true},"outputs":[],"source":["def train_epoch(model, optimizer,epoch):\n","    model.train()\n","    losses = 0\n","    for batch in tqdm(train_dataloader, desc=f\"Epoch:{epoch}\",total=len(train_dataloader), leave=False):\n","        input_ids = batch[0].to(device)\n","        attention_mask = batch[1].to(device)\n","        # token_type_ids = batch[2].to(device)\n","        labels = batch[2].to(device)\n","        # print(\"train: \",labels.shape,input_ids.shape,attention_mask.shape)\n","        utterance_wise_batch={}\n","        batch_size=input_ids.shape[0]\n","        utterance_input_ids=input_ids.view(MAX_UTTERANCES*batch_size,-1)\n","        utterance_attention_mask=attention_mask.view(MAX_UTTERANCES*batch_size,-1)\n","        # for i in range(BATCH_SIZE):\n","        #     for j in range(MAX_UTTERANCES):\n","        #         utterance_input_ids.append(input_ids[i][j])\n","        #         utterance_attention_mask.append(attention_mask[i][j])\n","        # print(len(utterance_wise_batch),utterance_wise_batch[0])\n","        # utterance_input_ids=torch.stack(utterance_input_ids)\n","        # utterance_attention_mask=torch.stack(utterance_attention_mask)\n","        utterance_wise_batch['input_ids']=utterance_input_ids\n","        utterance_wise_batch['attention_mask']=utterance_attention_mask\n","        # print(utterance_input_ids.shape,utterance_attention_mask.shape,len(utterance_wise_batch))\n","        optimizer.zero_grad()\n","        predicted = model(utterance_input_ids, utterance_attention_mask)\n","        # print(\"lables: \",labels.shape)\n","        predicted=predicted.view(-1,num_labels)\n","        labels=labels.view(-1)\n","        # print(\"train: \",predicted.shape,labels.shape)\n","        predicted_labels=predicted.argmax(dim=-1)\n","        # print(predicted_labels,labels)\n","        loss = criterion(predicted, labels)\n","        losses += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        del input_ids\n","        del attention_mask\n","        del labels\n","        del predicted\n","        del utterance_input_ids\n","        del utterance_attention_mask\n","        del utterance_wise_batch\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        break\n","    x = losses / len(list(train_dataloader))\n","    train_losses.append(x)\n","#     wandb.log({'epoch':epoch,'train_loss':x})\n","    tqdm.write(f\"Epoch:{epoch}, Avg Train Loss: {x}\")\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","    return x\n","\n","\n","\n","def evaluate(model,val_dataloader,name):\n","    with torch.no_grad():\n","        model.eval()\n","        losses = 0\n","\n","        all_labels = []\n","        for batch in tqdm(val_dataloader, desc=name,total=len(val_dataloader), leave=False):\n","            input_ids = batch[0].to(device)\n","            attention_mask = batch[1].to(device)\n","            labels = batch[2].to(device)\n","            utterance_wise_batch={}\n","            batch_size=input_ids.shape[0]\n","            utterance_input_ids=input_ids.view(MAX_UTTERANCES*batch_size,-1)\n","            utterance_attention_mask=attention_mask.view(MAX_UTTERANCES*batch_size,-1)\n","            utterance_wise_batch['input_ids']=utterance_input_ids\n","            utterance_wise_batch['attention_mask']=utterance_attention_mask\n","            predicted = model(utterance_input_ids, utterance_attention_mask)\n","            predicted=predicted.view(-1,num_labels)\n","            labels=labels.view(-1)\n","            loss = criterion(predicted, labels)\n","            losses += loss.item()\n","            predicted_labels = predicted.argmax(dim=-1)\n","            all_labels.extend(predicted_labels.cpu().numpy())\n","            del input_ids\n","            del attention_mask\n","            del labels\n","            del predicted\n","            del utterance_input_ids\n","            del utterance_attention_mask\n","            del utterance_wise_batch\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","            # break\n","        x = losses / len(list(val_dataloader))\n","        val_losses.append(x)\n","        tqdm.write(f\"Avg {name} Loss: {x}\")\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        return x, all_labels\n","    \n","\n"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:55:11.448969Z","iopub.status.busy":"2024-04-13T11:55:11.448663Z","iopub.status.idle":"2024-04-13T11:58:23.577323Z","shell.execute_reply":"2024-04-13T11:58:23.576333Z","shell.execute_reply.started":"2024-04-13T11:55:11.448943Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8155e8a54192468b9ad2af9f16f71531","version_major":2,"version_minor":0},"text/plain":["Epoch:1:   0%|          | 0/674 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:878: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:982.)\n","  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n"]},{"ename":"RuntimeError","evalue":"shape '[1000000, 1]' is invalid for input of size 40000","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     val_loss,all_labels \u001b[38;5;241m=\u001b[39m evaluate(model,val_dataloader\u001b[38;5;241m=\u001b[39mval_dataloader,name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(epoch\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n","Cell \u001b[1;32mIn[38], line 25\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, epoch)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print(utterance_input_ids.shape,utterance_attention_mask.shape,len(utterance_wise_batch))\u001b[39;00m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 25\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mutterance_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutterance_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# print(\"lables: \",labels.shape)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m predicted\u001b[38;5;241m=\u001b[39mpredicted\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,num_labels)\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[1;32mIn[37], line 23\u001b[0m, in \u001b[0;36mEmotionClassifier.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[0;32m     20\u001b[0m output_logits\u001b[38;5;241m=\u001b[39moutput_logits\u001b[38;5;241m.\u001b[39mview(batch_size,MAX_UTTERANCES\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta_labels) \u001b[38;5;66;03m# (2,25*10)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# print(\"OUTPUT: \",output_logits.shape) \u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# logits = self.linear(output_logits)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m logits\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mview(batch_size,MAX_UTTERANCES,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# print(\"logits: \",logits.shape)\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n","\u001b[1;31mRuntimeError\u001b[0m: shape '[1000000, 1]' is invalid for input of size 40000"]}],"source":["gc.collect()\n","torch.cuda.empty_cache()\n","for epoch in range(1, EPOCHS+1):\n","    train_loss = train_epoch(model, optimizer,epoch)\n","    val_loss,all_labels = evaluate(model,val_dataloader=val_dataloader,name='Val')\n","    if(epoch%1==0):\n","        torch.save(model, f\"{OUTPATH}modelM1_epoch{epoch}.pth\")\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:58:23.579139Z","iopub.status.busy":"2024-04-13T11:58:23.578769Z","iopub.status.idle":"2024-04-13T11:58:24.751519Z","shell.execute_reply":"2024-04-13T11:58:24.750621Z","shell.execute_reply.started":"2024-04-13T11:58:23.579113Z"},"trusted":true},"outputs":[],"source":["torch.save(model, f\"{OUTPATH}modelM1.pth\")\n","torch.save(tokenizer, f\"{OUTPATH}tokenizerM1.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:58:24.753299Z","iopub.status.busy":"2024-04-13T11:58:24.752914Z","iopub.status.idle":"2024-04-13T11:58:43.224885Z","shell.execute_reply":"2024-04-13T11:58:43.223824Z","shell.execute_reply.started":"2024-04-13T11:58:24.753239Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a5757e282354c1885a3d8a914b7d4d7","version_major":2,"version_minor":0},"text/plain":["Test:   0%|          | 0/85 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Avg Test Loss: 1.5519384145736694\n"]}],"source":["# load model\n","loaded_model = torch.load(f\"{OUTPATH}modelM1.pth\")\n","loaded_tokenizer = torch.load(f\"{OUTPATH}tokenizerM1.pth\")\n","\n","# test\n","x_test, y_test = get_data(PATH+'val_file.json')\n","test_dataset = Dataset(x_test,loaded_tokenizer,y_test)\n","test_dataloader=DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=False)\n","test_loss,all_labels = evaluate(loaded_model,test_dataloader,name='Test')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:58:43.233411Z","iopub.status.busy":"2024-04-13T11:58:43.233024Z","iopub.status.idle":"2024-04-13T11:58:43.244030Z","shell.execute_reply":"2024-04-13T11:58:43.243198Z","shell.execute_reply.started":"2024-04-13T11:58:43.233379Z"},"trusted":true},"outputs":[],"source":["y_test_list = np.array(y_test).flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(21075, (21075,), (843, 25))"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["len(y_test_list),np.array(all_labels).shape,np.array(y_test).shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["({3: 788, 7: 3200, 1: 1008, 5: 215, 4: 265, 2: 558, 0: 13782, 6: 1259},\n"," {7: 4215, 0: 16860},\n"," 21075,\n"," 21075)"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["dic1={}\n","dic2={}\n","count1=0\n","for i in y_test_list:\n","    count1+=1\n","    if i in dic1:\n","        dic1[i]+=1\n","    else:\n","        dic1[i]=1\n","\n","count2=0\n","for i in all_labels:\n","    count2+=1\n","    if i in dic2:\n","        dic2[i]+=1\n","    else:\n","        dic2[i]=1\n","\n","dic1,dic2,count1,count2"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-13T11:58:43.245445Z","iopub.status.busy":"2024-04-13T11:58:43.245130Z","iopub.status.idle":"2024-04-13T11:58:43.281156Z","shell.execute_reply":"2024-04-13T11:58:43.280151Z","shell.execute_reply.started":"2024-04-13T11:58:43.245421Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Test loss: 1.552\n","0.1687305441920074 0.6465387859620123\n","              precision    recall  f1-score   support\n","\n","           0       0.80      0.98      0.88     13782\n","           1       0.00      0.00      0.00      1008\n","           2       0.00      0.00      0.00       558\n","           3       0.00      0.00      0.00       788\n","           4       0.00      0.00      0.00       265\n","           5       0.00      0.00      0.00       215\n","           6       0.00      0.00      0.00      1259\n","           7       0.41      0.55      0.47      3200\n","\n","    accuracy                           0.72     21075\n","   macro avg       0.15      0.19      0.17     21075\n","weighted avg       0.59      0.72      0.65     21075\n","\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["print(f\"Test loss: {test_loss:.3f}\")\n","f1_sccore = f1_score(y_test_list, all_labels, average='weighted')\n","f1_macro = f1_score(y_test_list, all_labels, average='macro')\n","print(f1_macro,f1_sccore)\n","print(classification_report(y_test_list, all_labels))"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4784361,"sourceId":8101413,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
