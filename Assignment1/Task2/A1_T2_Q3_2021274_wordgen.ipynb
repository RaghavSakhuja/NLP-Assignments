{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7550323,"sourceType":"datasetVersion","datasetId":4397408}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import random\nimport numpy as np\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-05T17:03:42.468054Z","iopub.execute_input":"2024-02-05T17:03:42.468805Z","iopub.status.idle":"2024-02-05T17:03:43.709975Z","shell.execute_reply.started":"2024-02-05T17:03:42.468766Z","shell.execute_reply":"2024-02-05T17:03:43.708259Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"count=50\ntypes=[\"No\",\"Laplace\",\"KneserNey\"]\n# types=[\"KneserNey\"]\nemotions=[\"sadness\",\"joy\",\"love\",\"anger\",\"fear\",\"surprise\"]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-05T17:03:43.713138Z","iopub.execute_input":"2024-02-05T17:03:43.713834Z","iopub.status.idle":"2024-02-05T17:03:43.719532Z","shell.execute_reply.started":"2024-02-05T17:03:43.713788Z","shell.execute_reply":"2024-02-05T17:03:43.718526Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def beta_unigrams(emotion):\n    df=pd.read_csv(\"/kaggle/input/word-gen/unigram_emotion.csv\")\n    df=df[emotion]\n    df=df.to_numpy()\n    row=df\n    row=row/sum(row)\n#     print(len(row),sum(row),\"row_uni\")\n    return row\n\ndef p_first(corpus,uniquewords) :\n    with open(corpus) as f:\n        lines = f.readlines()\n    word={}\n    total=0\n    for line in lines:\n        words=line.split()\n        if words[0] in word:\n            word[words[0]]+=1\n            total+=1\n        else:\n            word[words[0]]=1\n            total+=1\n        for j in words:\n            if(j not in word):\n                word[j]=1\n                total+=1\n    probs=[]\n    # print(len(word.keys()))\n    for i in uniquewords:\n        if i in word:\n            probs.append(word[i]/total)\n        else:\n            probs.append(0)\n    \n#     print(total,len(probs),sum(probs),\"first_prob\")\n    # print(probs)\n    probs=np.array(probs)\n    return probs\n\nunigram_emotions=[beta_unigrams(i) for i in emotions]\n\nbigram_sadness=pd.read_csv(\"/kaggle/input/word-gen/sadness_scores.csv\")\nbigram_joy=pd.read_csv(\"/kaggle/input/word-gen/joy_scores.csv\")\nbigram_love=pd.read_csv(\"/kaggle/input/word-gen/love_scores.csv\")\nbigram_anger=pd.read_csv(\"/kaggle/input/word-gen/anger_scores.csv\")\nbigram_fear=pd.read_csv(\"/kaggle/input/word-gen/fear_scores.csv\")\nbigram_surprise=pd.read_csv(\"/kaggle/input/word-gen/surprise_scores.csv\")\n\nNo_smoothing=pd.read_csv(\"/kaggle/input/word-gen/No_Smoothing.csv\")\nLaplace_smoothing=pd.read_csv(\"/kaggle/input/word-gen/Laplace_Smoothing.csv\")\nKneserNey_smoothing=pd.read_csv(\"/kaggle/input/word-gen/KneserNey_Smoothing.csv\")\n\nuniquewords=KneserNey_smoothing[\"Unnamed: 0\"]\nuniquewords=uniquewords.to_numpy()\nuniquewords=list(uniquewords)\n\n\nfirst_prob=p_first(\"/kaggle/input/word-gen/corpus.txt\",uniquewords)\n\n\ndef generate_sentences(sentence_length,alpha,beta,uniquewords,emotion,type):\n        global first_prob,unigram_emotions\n        # r=random.choice(self.uniquewords)\n        # first_word=random.choise(uniquewords,weights=first_generator(0),k=1)[0]\n        beta_unigram_dist=unigram_emotions[emotions.index(emotion)]\n        first_prob_dist=first_prob*beta+beta_unigram_dist*(1-beta)\n        # print(len(first_prob_dist),first_prob_dist.dtype,sum(first_prob_dist))\n        first_word=random.choices(uniquewords,weights=first_prob_dist,k=1)[0]\n        s=first_word+\" \"\n        k=uniquewords.index(first_word)\n        for i in range(sentence_length-1):\n            beta_unigram_dist=unigram_emotions[emotions.index(emotion)]\n            beta_bigram_dist=beta_bigrams(k,emotion)\n            smoothed_probabs=prob_distribution(k,type)\n            prob_dist=alpha*smoothed_probabs+(1-alpha/2)*beta_unigram_dist+(1-alpha/2)*beta_bigram_dist\n            prob_dist=prob_dist/sum(prob_dist)\n#             prob_dist=alpha*smoothed_probabs+(1-alpha)*beta_bigram_dist\n#             prob_dist=alpha*smoothed_probabs+(1-alpha)*beta_unigram_dist\n#             print(sum(prob_dist),\"distribution\")\n            # prob_dist=alpha*smoothed_probabs+(1-alpha)*beta_unigram_dist\n            w=random.choices(uniquewords,weights=prob_dist,k=1)[0]\n            # w=random.choices(self.uniquewords,weights=self.co_matrix[k],k=1)[0]\n            s+=w+\" \"\n            k=uniquewords.index(w)\n        return s\n\n\n\ndef beta_bigrams(k,emotion):\n    global bigram_sadness,bigram_joy,bigram_love,bigram_anger,bigram_fear,bigram_surprise\n    idx=k\n    #row idx of df\n    if(emotion==\"sadness\"):\n        df=bigram_sadness\n    elif(emotion==\"joy\"):\n        df=bigram_joy\n    elif(emotion==\"love\"):\n        df=bigram_love\n    elif(emotion==\"anger\"):\n        df=bigram_anger\n    elif(emotion==\"fear\"):\n        df=bigram_fear\n    elif(emotion==\"surprise\"):\n        df=bigram_surprise\n    \n\n    row=df.iloc[idx]\n    row=row.to_numpy()\n    row=row[1:]\n    row=row/sum(row)\n    # print(len(row))\n    return row\n\ndef prob_distribution(k,type):\n    global No_smoothing,Laplace_smoothing,KneserNey_smoothing\n\n    if(type==\"No\"):\n        df=No_smoothing\n    elif(type==\"Laplace\"):\n        df=Laplace_smoothing\n    elif(type==\"KneserNey\"):\n        df=KneserNey_smoothing\n\n    idx=k\n    #row idx of df\n    row=df.iloc[idx]\n    row=row.to_numpy()\n    row=row[1:]\n    # print(row)\n    row=row/sum(row)\n    # print(len(row))\n    return row\n\n    # print(uniquewords)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-05T17:03:43.721243Z","iopub.execute_input":"2024-02-05T17:03:43.721874Z","iopub.status.idle":"2024-02-05T17:06:08.441654Z","shell.execute_reply.started":"2024-02-05T17:03:43.721835Z","shell.execute_reply":"2024-02-05T17:06:08.440248Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# print(generate_sentences(10,0.5,uniquewords,\"joy\"))\n\nfor j in emotions:\n    for k in types:\n        sentences=[]\n        for i in range(count):\n            l=random.randint(8,30)\n            sentences.append(generate_sentences(l,0.5,0.9,uniquewords,j,k))    \n        all=\"\\n\".join(sentences)\n        print(j,k)\n        with open(\"gen_\"+j+\"_\"+k+\".txt\",\"w\") as f:\n            f.write(all)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-05T17:06:08.444710Z","iopub.execute_input":"2024-02-05T17:06:08.445456Z","iopub.status.idle":"2024-02-05T17:11:32.541354Z","shell.execute_reply.started":"2024-02-05T17:06:08.445420Z","shell.execute_reply":"2024-02-05T17:11:32.540106Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"sadness No\nsadness Laplace\nsadness KneserNey\njoy No\njoy Laplace\njoy KneserNey\nlove No\nlove Laplace\nlove KneserNey\nanger No\nanger Laplace\nanger KneserNey\nfear No\nfear Laplace\nfear KneserNey\nsurprise No\nsurprise Laplace\nsurprise KneserNey\n","output_type":"stream"}]}]}