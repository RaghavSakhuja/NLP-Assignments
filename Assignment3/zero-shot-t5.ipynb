{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-24T16:17:39.124653Z","iopub.status.busy":"2024-03-24T16:17:39.124250Z","iopub.status.idle":"2024-03-24T16:17:39.148688Z","shell.execute_reply":"2024-03-24T16:17:39.147708Z","shell.execute_reply.started":"2024-03-24T16:17:39.124622Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:From c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n","\n"]}],"source":["\n","import json\n","import torch\n","import logging\n","import numpy as np \n","import pandas as pd \n","import evaluate\n","from evaluate import load\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:17:39.151018Z","iopub.status.busy":"2024-03-24T16:17:39.150302Z","iopub.status.idle":"2024-03-24T16:17:39.156306Z","shell.execute_reply":"2024-03-24T16:17:39.155121Z","shell.execute_reply.started":"2024-03-24T16:17:39.150984Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from transformers import T5Tokenizer,T5ForConditionalGeneration\n","import json"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"]}],"source":["device=\"\"\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    logging.info(f'Using GPU: {torch.cuda.get_device_name()}')\n","    print(f'Using GPU: {torch.cuda.get_device_name()}')\n","else:\n","    device = torch.device(\"cpu\")\n","    logging.info('Using CPU')\n","    print('Using CPU')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bertscore = load(\"bertscore\")\n","meteor = evaluate.load(\"meteor\")\n","bleu = evaluate.load(\"bleu\")"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n","model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\").to(device)"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["file_path = \"data/Task2/\""]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["testing_data = []\n","validation_data = []"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":91,"metadata":{"trusted":true},"outputs":[],"source":["# load json\n","with open(file_path + \"test.json\") as f:\n","    testing_data = json.load(f)\n","    testing_data = testing_data[\"translation\"]\n","\n","with open(file_path + \"validation_data.json\") as f:\n","    validation_data = json.load(f)\n","    validation_data = validation_data[\"translation\"]\n","\n","\n","en_test_data = []\n","en_val_data = []\n","\n","de_test_data = []\n","de_val_data = []\n","\n","for i in range(len(testing_data)):\n","    en_test_data.append(testing_data[i][\"en\"])\n","    de_test_data.append(testing_data[i][\"de\"])\n","for i in range(len(validation_data)):\n","    de_val_data.append(validation_data[i][\"de\"])\n","    en_val_data.append(validation_data[i][\"en\"])\n","\n","\n"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[],"source":["\n","# en_test_data=en_test_data[:100]\n","# de_test_data=de_test_data[:100]\n","# en_val_data=en_val_data[:100]\n","# de_val_data=de_val_data[:100]\n","\n"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[{"data":{"text/plain":["(2998, 2998, 2168, 2168)"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["len(en_test_data),len(de_test_data),len(en_val_data),len(de_val_data)"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":true},"outputs":[],"source":["prefix = \"translate English to German: \"\n","test_model_input=[]\n","val_model_input=[]\n","for i in range(len(en_test_data)):\n","    test_model_input.append(prefix + en_test_data[i])\n","for i in range(len(en_val_data)):\n","    val_model_input.append(prefix + en_val_data[i])\n","\n"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["100\n"]},{"data":{"text/plain":["100"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["test_output = []\n","\n","j=0;\n","for i in test_model_input:\n","    input_ids = tokenizer(i, return_tensors=\"pt\").input_ids.to(device)\n","    output = model.generate(input_ids, max_length=256, num_beams=5, early_stopping=True)\n","    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n","    test_output.append(decoded_output)\n","    j+=1\n","    if( j%10 == 0):\n","        print(j)\n","\n","len(test_output)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["# i=50\n","# print(en_test_data[i],de_test_data[i],test_output[i],sep=\"\\n\")\n","# input_ids = tokenizer(test_model_input[i], return_tensors=\"pt\").input_ids.to(device)\n","# output = model.generate(input_ids, max_length=256)\n","# decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n","# decoded_output"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'bleu': 0.41678909373773215, 'precisions': [0.6959890610756609, 0.47659980897803245, 0.35456369107321967, 0.26610348468849], 'brevity_penalty': 0.9909256523892572, 'length_ratio': 0.99096657633243, 'translation_length': 2194, 'reference_length': 2214}\n"]}],"source":["\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = bleu.compute(predictions=test_output, references=de_test_data)\n","print(results)"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'meteor': 0.6658749178164521}\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to\n","[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["# meteor\n","results = meteor.compute(predictions=test_output, references=de_test_data)\n","print(results)"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["tokenizer_config.json: 100%|██████████| 49.0/49.0 [00:00<?, ?B/s]\n","c:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\acer\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","config.json: 100%|██████████| 625/625 [00:00<00:00, 625kB/s]\n","vocab.txt: 100%|██████████| 996k/996k [00:00<00:00, 1.18MB/s]\n","tokenizer.json: 100%|██████████| 1.96M/1.96M [00:01<00:00, 1.82MB/s]\n","model.safetensors: 100%|██████████| 714M/714M [00:33<00:00, 21.4MB/s] \n"]}],"source":["\n","results = bertscore.compute(predictions=test_output, references=de_test_data, lang=\"de\")\n"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.8949023234844208\n","0.8967794805765152\n","0.8932868671417237\n"]}],"source":["print(np.average(results[\"f1\"]))\n","print(np.average(results[\"precision\"]))\n","print(np.average(results[\"recall\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_output = []\n","\n","j=0;\n","for i in val_model_input:\n","    input_ids = tokenizer(i, return_tensors=\"pt\").input_ids.to(device)\n","    output = model.generate(input_ids, max_length=256, num_beams=5, early_stopping=True)\n","    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n","    val_output.append(decoded_output)\n","    j+=1\n","    if( j%10 == 0):\n","        print(j)\n","\n","len(val_output)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","results = bleu.compute(predictions=val_output, references=de_val_data)\n","print(results)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = meteor.compute(predictions=val_output, references=de_val_data)\n","print(results)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results = bertscore.compute(predictions=val_output, references=de_val_data, lang=\"de\")\n","print(np.average(results[\"f1\"]))\n","print(np.average(results[\"precision\"]))\n","print(np.average(results[\"recall\"]))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30673,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
