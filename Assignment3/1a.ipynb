{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertForSequenceClassification\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from transformers import *\n",
    "import torch.optim as optim\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Set your wandb API key\n",
    "wandb_api_key = \"e62fc492915628e64fcac9c082089ffed84dc72d\"\n",
    "\n",
    "# Log in to wandb using the API key\n",
    "wandb.login(key=wandb_api_key)\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project=\"nlp3-1a\")\n",
    "\n",
    "# Now you can use wandb for logging during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path=\"/kaggle/input/assignment-3/A3_task1_data_files/train.csv\"\n",
    "val_path=\"/kaggle/input/assignment-3/A3_task1_data_files/dev.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPairDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data=data\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence_pair = self.data.iloc[idx, 1:3].values.tolist()\n",
    "        \n",
    "        encoded_pair = self.tokenizer(sentence_pair[0], sentence_pair[1],\n",
    "                                      add_special_tokens=True, \n",
    "                                      padding='max_length', \n",
    "                                      truncation=True, \n",
    "                                      return_tensors='pt')\n",
    "        input_ids = encoded_pair['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded_pair['attention_mask'].squeeze(0)\n",
    "        labels = torch.tensor(self.data.iloc[idx, 0])\n",
    "        return input_ids,attention_mask,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "DEVICE='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "NUM_EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloaderbuilder(filepath,batch_size):\n",
    "    data=pd.read_csv(filepath,sep='\\t').dropna()\n",
    "    dataset=TextPairDataset(data,tokenizer)\n",
    "    loader=DataLoader(dataset,batch_size=batch_size,shuffle=False)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=dataloaderbuilder(train_path,BATCH_SIZE)\n",
    "val_dataloader=dataloaderbuilder(val_path,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotgraph(train_losses,val_losses):\n",
    "    x=[i+1 for i in range(NUM_EPOCHS)]\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss per Epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss_graph.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model=BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(model.config.hidden_size, 1),\n",
    "    nn.Sigmoid()  # Output float between 0 and 1\n",
    ")\n",
    "model.to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "loss_fn=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "def train_epoch(model, optimizer,epoch):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    progress = tqdm(train_dataloader, desc=f\"Epoch:{epoch}\",total=len(train_dataloader), leave=False)\n",
    "    i=0\n",
    "    for batch in progress:\n",
    "        input_ids = batch[0].to(DEVICE)\n",
    "        attention_mask = batch[1].to(DEVICE)\n",
    "        labels = batch[2].to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        logits=model(input_ids,attention_mask=attention_mask).logits.to(torch.float64).view(-1)*5\n",
    "        loss=loss_fn(logits,labels)\n",
    "        losses += loss.item()\n",
    "#         print(i,loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del input_ids\n",
    "        del attention_mask\n",
    "        del labels\n",
    "        del logits\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        progress.set_postfix({'training_loss': f'{loss.item()/len(batch):.3f}'})\n",
    "    x = losses / len(list(train_dataloader))\n",
    "    train_losses.append(x)\n",
    "    wandb.log({'epoch':epoch,'train_loss':x})\n",
    "    tqdm.write(f\"Epoch:{epoch}, Avg Train Loss: {x}\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model,epoch):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        losses = 0\n",
    "        batch_pred=[]\n",
    "        batch_truth=[]\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch[0].to(DEVICE)\n",
    "            attention_mask = batch[1].to(DEVICE)\n",
    "            labels = batch[2].to(DEVICE)\n",
    "\n",
    "            logits=model(input_ids,attention_mask=attention_mask).logits.to(torch.float64).view(-1)*5\n",
    "            batch_pred.append(logits)\n",
    "            batch_truth.append(labels)\n",
    "\n",
    "            loss=loss_fn(logits,labels)\n",
    "            losses += loss.item()\n",
    "            del input_ids\n",
    "            del attention_mask\n",
    "            del labels\n",
    "            del logits\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        predicted_scores=torch.cat(batch_pred)\n",
    "        ground_truth_labels=torch.cat(batch_truth)\n",
    "        pearson_coefficient, _ = pearsonr(predicted_scores.cpu(), ground_truth_labels.cpu())\n",
    "        print(\"Pearson Correlation Coefficient:\", pearson_coefficient)\n",
    "\n",
    "        x = losses / len(list(val_dataloader))\n",
    "        val_losses.append(x)\n",
    "        wandb.log({'epoch':epoch,'val_loss':x,'val_pearson':pearson_coefficient})\n",
    "        del predicted_scores\n",
    "        del ground_truth_labels\n",
    "        del pearson_coefficient\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    train_loss = train_epoch(model, optimizer,epoch)\n",
    "    val_loss = evaluate(model,epoch)\n",
    "    torch.save(model.state_dict(), f\"model1A_epoch_{epoch}.pth\")\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loading val\n",
    "# def evaluate(model):\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "#         losses = 0\n",
    "#         batch_pred=[]\n",
    "#         batch_truth=[]\n",
    "#         for batch in val_dataloader:\n",
    "#             input_ids = batch[0].to(DEVICE)\n",
    "#             attention_mask = batch[1].to(DEVICE)\n",
    "#             labels = batch[2].to(DEVICE)\n",
    "\n",
    "#             logits=model(input_ids,attention_mask=attention_mask).logits.to(torch.float64).view(-1)*5\n",
    "#             batch_pred.append(logits)\n",
    "#             batch_truth.append(labels)\n",
    "#             loss=loss_fn(logits,labels)\n",
    "#             losses += loss.item()\n",
    "#             del input_ids\n",
    "#             del attention_mask\n",
    "#             del labels\n",
    "#             del logits\n",
    "#             gc.collect()\n",
    "#             torch.cuda.empty_cache()\n",
    "#         predicted_scores=torch.cat(batch_pred)\n",
    "#         ground_truth_labels=torch.cat(batch_truth)\n",
    "#         pearson_coefficient, _ = pearsonr(predicted_scores.cpu(), ground_truth_labels.cpu())\n",
    "#         print(\"Pearson Correlation Coefficient:\", pearson_coefficient)\n",
    "#         print(predicted_scores)\n",
    "#         print(ground_truth_labels)\n",
    "#         x = losses / len(list(val_dataloader))\n",
    "#         val_losses.append(x)\n",
    "\n",
    "#         return x\n",
    "# PATH=\"/kaggle/input/831asaves/model1A_epoch_10.pth\"\n",
    "# from torch import nn\n",
    "# model=BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Linear(model.config.hidden_size, 1),\n",
    "#     nn.Sigmoid()  # Output float between 0 and 1\n",
    "# )\n",
    "# model.load_state_dict(torch.load(PATH))\n",
    "# model.to(DEVICE)\n",
    "# loss_fn=nn.MSELoss()\n",
    "# val_loss = evaluate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
